
KUBERNETES :
----------
Kubernetes is an open source system for automating deployment, scaling, and management of containerized applications.
A system which manages or orchestrates containers are called orchestration tool. Kubernetes is one of them which was open sourced by Google in 2014
and maintained by CNCF.
Kubernetes in greek is known as Helmsman (which represents ship controlling wheel(K8's symbol)). K8s -> 8 letters between K and s of Kubernetes.

{{ NOTE - Node here is Laptop (physical machine) and EC2 (VM's) }}


PROBLEMS SOLVED BY KUBERNETES -> FEATURES INVOLVED :
--------------------------------------------------
1]	When an Containerized application goes down or an Node goes down, Kubernetes monitor's each node, identifies and brings up the unhealthy ones.
	-> Monitoring
	-> Self-Healing
	-> High Availability

2] 	Handling sudden increase in workloads
	-> Load Balancing
	-> Auto Scaling
	-> Automatic bin packing
	
3] 	Releasing updates multiple time causes downtime of the container's.
	-> Rolling and Canary Deployments
	-> Automatic Rollout and Rollback
	
4] 	Secret and Configuration management


ALTERNATIVE TO KUBERNETES :
-------------------------
Docker Swarm
Mesos
AWS ECS, EKS


KUBERNETES ARCHITECTURE :
-----------------------
{{NOTE - Pods are created by Control manager & They are managed by Kubelet}}

1]	CLUSTER :
	-------
	In Kubernetes (K8s), a cluster refers to a group of machines (physical or virtual) that work together to run containerized applications.
	These machines are connected and managed by the Kubernetes control plane, which ensures that the desired state of the cluster is maintained.
	The cluster is the core of Kubernetes and allows for efficient and scalable deployment of containerized applications across multiple machines.
	
	Basically, We have multiple worker nodes (each node is an VM or EC2). Multi worker node forms Worker plan, These multi worker nodes are
	controlled by Control plan (Master node), Both Control and Worker plan forms a Cluster.
	
2]	MASTER NODE COMPONENTS :
	----------------------
	Control plan will be in constant contact with the worker node to make sure cluster is running with all the configuration.
	Where as, Worker node is responsible for managing the application within pod.

	a] KUBE API-SERVER :
		It is an Control plane component which exposes Kubernetes API, through which we end user use CLI or SDK to communicate (shedule pod,etc)
		with K8s. So, It is called as the front-end of K8s.
		
	b] ETCD :
		It is an simple key-value storage which is used to store cluster data (no of nodes, pods, containers and their state, etc,..)
		It is always recommended to have backup plan for etcd so we can restore during emergencies.
		For security reasons only Api-server can access ETCD, not other components can interact directly.
		It has an important feature called Watch-API, which monitor's the key's (resources) and updates any changes to Client through Api-server.
		
	c] SCHEDULAR :
		It helps to schedule pods in various nodes based on resource utilization (Hardware constraints (Memory, CPU), Software constraints).
		Once an node is selected for pod scheduling it then calls Api-server.
		
	d] KUBE CONTROL MANAGER :
		If any changes in configurations occurs (eg: Replacing of Images in which pods are running or changing parameters in configuration YAML file, etc)
		The Controller spots the changes and works on desired changes. Different types of Controller :
			a] Replication Controller - Ensures the desired no of pods are running.
			b] Node Controller - Monitor's the health of Node and reports to Cluster (When Node comes online or goes unresponsive)
			c] Endpoint Controller, etc - It connects the pods and services to populate the end points. And responsible for managing the Endpoints.
		
3]	WORKER NODE COMPONENTS :
	----------------------
	a] CONTAINER-RUNTIME :
		In order to run an container from an image we need Container-Runtime. Docker, ContainerD, Raket are some examples.
		But Docker is the most common one.
		
	b] KUBELET :
		It is an agent which runs in every worker node. It ensures that containers are running within the pods.
		It is also responsible for registering a node to a cluster, and sending reports on pod status and resource utilization.
		It continously looks for any changes in pod configuration file from Api-server and ensures that the pod and containers are running in healthy
		desired state.
		
	c] KUBE-PROXY :
		Kube-Proxy is a network proxy that runs on each node in a Kubernetes cluster.
		It is responsible for maintaining network connectivity between services and pods.
		Kube-Proxy is the one which creates IP address for new pods.
		

KUBERNETES WORKFLOW :
-------------------
In order to create 2 instance of applications, We create an configuration file for it. Then, we send it to Api-server directly or through CLI.
->	Api-server runs the Config file through Schedular. The Schedular selects the worker node on which new nodes to be created based on
	config file and resource availability.
->	Api-server simultaneously sends the Config file to ETCD, In which the data and status is stored in an Key-value pair.
->	Once, Schedular selects the worker node. The Control-Manager sends an object Config file to Kubelet through Api-server
	inorder to create the desired objects.
->	Kubelet after receving the object Config file, Creates the objects in their desired state in the Worker node.
->	Whenever, the pod status is changed like killed,etc,.. The Kubelet updates the status to ETCD through Api-server.
->	The Watch functionality of ETCD monitor's the Current and Desired status of the objects.
->	If both status doesn't match then control loop which runs through control manager responds to these status changes and work towards
	achieving the desired state.
->	After application is deployed, If we wanted to make any request to the application, The Kube-Proxy takes our request and forwards it to
	the corresponding pods.
	
	
ENVIRONMENT SETUP :
-----------------
Local Setup - Minikube or Kind
Production - Kubeadm, Rancher, EKS, ECS, etc,..


KUBECTL SYNTAX :
--------------
kubectl [command] [type] [name] [flag]-> options like -f,-o, etc,..
 |			|		|		|-> Name of the Resource
 |-> CLI	|		|-> Resource types like pods,services,etc
			|-> create,get,delete,decribe,etc,..
			
(e.g): kubectl get pods my-pod -o yaml


YAML :
----
YAML stands for YAML Ain't Markup language (its not like HTML,XML)
It's an serialization language (Commonly agreed language which is used to transfer data in the form of binary's(0's & 1's) over Network)
.....


PODS :
----
Kubernetes pods are the smallest deployable units that encapsulate one or more containers and provide shared resources.
They are managed by controllers and provide features like scalability, fault tolerance, and simplified management.
1]	Sidecar container / Helper container :
	In certain scenario's we deploy multiple containers in single pod.
	(e.g): When an application is deployed within a container, Its dependent configurations are stored in another database container.
	Then a third container is deployed to manage refresh service related tasks (getting configuration updates from github for every 1hour ,etc).
	These additional containers are called Sidecar / Helper containers. Which needed to be deleted / deployed along with application container.
2]	Scaling :
	Since, Containers in a pod share same network and volume. All of them can be scaled up or down together.
3]	Pod Communication :
	When a pod is created, An IP address and Set of port ranges are assigned to each pods. With this we can run same application in same port on
	different pods of same node. Every container in a pod shares same IP address and Network space. So, Inter-communication between containers within
	a pod use localhost and for communication to containers in other pod use IP address.
4]	Pod Creation :
	Single resource is easy to create with kubectl commands, but for multiple resources its better to go with YAML files (Configuration / Manifest).
5] 	Commands used :
	kubectl run nginx-pod --image=nginx
	kubectl get pods
	kubectl api-resources | grep pods
	kubectl apply -f nginx-pod.yaml								-> Create, Update resources through yaml
	kubectl delete pod nginx-pod1
	kubectl get pods -l team=integration
	kubectl get pods -l team=integration, app=todo
	kubectl get pod nginx-pod1 -o wide
	kubectl get pod nginx-pod1 -o yaml
	kubectl describe pod nginx-pod1
	kubectl exec -it nginx-pod1 -- bash							-> Entering a pod for debuging
	exit														-> For coming out of -it mode
	kubectl exec -it nginx-pod1 -c nginx-container -- bash		-> Entering a container within a pod for debuging
	kubectl port-forward nginx-pod1 8083:80						-> Through this we can access application (which is inside pod) from internet
	kubectl logs nginx-pod1										-> For logs
	kubectl delete -f nginx-pod.yaml							-> Delete resources through yaml
	kubectl delete pod nginx-pod
	
{{ NOTE - When we make changes in .yaml(pod name) and apply it, It creates a new pod, It doesn't modify existing pod}}
	
	
SELF-HEALING & HIGH AVAILABILITY :
--------------------------------
High Availability (HA) in Kubernetes is crucial for ensuring that applications and services remain operational and accessible at all times,
minimizing downtime. HA involves designing a system where the failure of a single component does not lead to the overall system's downfall.
1] 	Replica :
	High Availability (If one of location,node,pod,etc goes down but the application runs in other locations,nodes,pod,etc )
	& Self-Healing (When a pod goes down, it brings up another in its place) can be achieved through Replicas in Kubernetes.
2]	Commands used :
	kubectl api-resources | grep replicaset
	kubectl apply -f nginx-replicaset.yaml
	kubectl get replicaset
	kubectl get rs
	kubectl get pods
	kubectl delete po nginx-replicaset-6lg9t					-> By deleting an pod, we conclude that replicaset brings up required no of pods.
	kubectl get nodes
	minikube node add --worker -p local-cluster					-> Adding an additional node
	kubectl get po -o wide
	minikube node delete local-cluster-m02 -p local-cluster		-> By deleting node, we conclude that pods in deleted node is re-created in other
																   node, which resulting in High Availability.
	kubectl get po -o wide
	kubectl delete all --all									-> Deletes all resources unconditionly.


ROLLOUT & ROLLBACK :
------------------
1]	Rollout / Rolling Updates :
	Rolling updates are the default strategy in Kubernetes. They ensure that the new version of the application is deployed gradually,
	without causing downtime. This is achieved by scaling up the new version and scaling down the old version simultaneously.
2]	Rollbacks :
	Rollbacks are used to revert to a previous version of the application if the new version is not stable. This ensures minimal downtime and
	maintains the overall availability of the application.
3]	Deployment :
	Rollout and Rollback can be automated using Deployment object in Kubernetes.
	
{{ NOTE - Hierarchy in an Deployment is Deployment -> Replicaset -> Pod -> Container }}

4]	Scaling up and down :
	Scaling can be achieved by two ways -
		a] By changing replicas in Deployment.yaml and applying them
		b] By using scale option in commands

5]	Commands used :
	kubectl apply -f nginx-deployment.yaml
	kubectl get all
	kubectl get po --show-labels								-> To get Labels associated with the objects
	kubectl apply -f nginx-deployment.yaml
		-> from { replicas: 2 } to { replicas: 4 }
	kubectl scale --replicas=4 deployment/nginx-deployment		-> Scaling can be done in commands and through changing deployment.yaml files
	
{{ NOTE - Changing replicas through command doesn't change replicas inside deployment.yaml files,
          So, Its preferred to use yaml files during works }}
		  
	kubectl apply -f nginx-deployment.yaml
		-> from { image: nginx:latest } to { image: nginx:1.21.3 }
	kubectl set image deployment/nginx-deployment nginx-container=nginx:1.21	-> Changing image through command
	
{{ NOTE - After applying old image version (Rollback), New Replica is created instead of altering the old one.
          Kubernetes stores upto 10 replicas to store, so that we can use them for Rollback's and Rollout's.
		  If needed we can change the no of replicas stored aswell }}
		  
	kubectl decribe <pod_name>
	kubectl rollout history deployment/nginx-deployment			-> Gives history of rollout's ( No of revesion represents no of rollout's &
																   Its best practice to give change of cause during rollout )
	kubectl set image deployment/nginx-deployment nginx-container=nginx:1.21 --record	-> Through this change of cause is recorded
	kubectl apply -f nginx-deployment.yaml
		-> New annotation: section added in metadata: section	-> Recording change of cause through deployment.yaml file
	kubectl rollout history deployment/nginx-deployment
	kubectl rollout undo deployment/nginx-deployment --to-revision=1 	-> By this we can rollout to required previous revisions
	kubectl rollout status deployment/nginx-deployment			-> Checks status of rollout


SERVICES :
--------
Pods are created with their own network, Hence we cannot access a pod from outside of a cluster, we can access them from only within using IP address.
(e.g : When we wanted to access an application within an pod, we use IP address to connect (within a cluster),
	   But same IP address cannot be used from outside)
But, note that during replica increase or decrease, rollout and rollback, new pods are always created. Hence, IP address also changes.

{{ NOTE - Port-forwarding is used to access pods for debugging purposes}}

Services are here to solve the above issue. Services are responsible for abstracting IP address from the pod.
When an service is created, It has a stable IP address assigned and it is linked to an pod.
By connecting to the service we can access the pod, even when pod IP address changes. Service takes care of internal pod mappings.

1]	Advantages of Service :
	a] Load-balancing - When there is more replica of the application and we try to connect to them, Service takes care of Load-balancing.
	
{{ NOTE - All service provides Load balancing }}
	
	b] Service discovery
	c] Zero downtime deployments
	
2]	Types of Services :
	a] ClusterIP service :
	This Service exposes the pod to IP addresses internal to cluster. i.e, Only objects internal to cluster can access this service and
	its associated pods. ClusterIP service is the default service associated in kubernetes.
	b] Multi-port service :
	When we have multiple containers in a pod and want to expose them, we use Multi-port service.
	
{{ NOTE - In Multi-port service, we must add a name feild for each container ports under port feild }}

	c] NodePort service :
	In order to expose our pod and its applications to outside world we use NodePort service. NodePort service has a static constant port (NodePort),
	By using Cluster IP address along with NodePort we can access the pods. NodePort service act like ClusterIP service and redirects traffic
	to pods. NodePort ranges from 30000 - 32767.
	d] LoadBalancer service :
	It's not secure to use NodePort service in production since we use node IP for its connection and when we restart node, IP address changes.
	Hence we have LoadBalancer service which behaves as NodePort, but it access the LoadBalancer of cloud provider for security.

3] 	Commands used :
	kubectl api-resources | grep services
	kubectl apply -f nginx-service.yaml
	kubectl get svc
	kubectl get pods
	kubectl apply -f nginx-deployment.yaml
	kubectl get pods
	kubectl exec -it nginx-deployment-6c4f6d48b8-2fw7k -- sh			-> Entering pod and checking wheather we can access the application
		-> curl <Cluster IP address>:8082
		-> curl nginx-service:8082
		-> exit
	kubectl port-forward service/nginx-service 8083:8082				-> Using port-forward and connecting from Internet
		-> In browser localhost:8083
		-> ctrl+c
	kubectl exec -it nginx-deployment-6c4f6d48b8-2fw7k -- sh
		-> Executed a script for continous curl nginx-service:8082
	kubectl logs nginx-deployment-6c4f6d48b8-2fw7k -f					-> To monitor logs
	
{{ NOTE - While doing Port-forwarding Load-balancing doesn't work }}

	kubectl get endpoints												-> To check what are all pods associated with the service (Endpoints)
	kubectl describe service/nginx-service
	kubectl apply -f nginx-NodePort_service.yaml
	kubectl get svc
	minikube ip -p local-clus											-> Get cluster IP address
		-> In Internet http://192.168.67.2:30000/
	minikube service nginx-service -p local-clus						-> Acessing Nodeport service from Internet from CLI
	kubectl apply -f nginx-NodePort_service.yaml
		-> By changing ( type: NodePort to type: LoadBalancer )
	kubectl get svc
	minikube service nginx-service -p local-clus						


INGRESS :
-------
1]	NodePort summary :
	One of the way to expose our application to outside of cluster is through NodePort service. In it we are opening an port on each node and
	exposing it to outside of cluster. The user uses node IP address along with NodePort inorder to connect, then the load is forward to
	the respective pods.
	
	Cons in NodePort service :
	a] The port ranges from only 30000 - 32767
	b] Node IP address may change after restart
	c] It's not secure to open a port in node.

2]	Cons in LoadBalancer service :
	Above Cons are resolved throgh LoadBalancer service,
	a] But it can be used only if we you have cloud operated environment like Google kubernetes engine, EKS, etc,.
	If we in bare metal we need to setup endpoints with proxy server,etc,.
	b] When we create LoadBalancer service, A LoadBalancer is created in cloud and we need to pay of it. And its costly
	Managing and Paying for multiple Load Balancer is costly and hard.
	
3]	Ingress :
	It will allow us to have single Load Balancer for multiple service and Route the load to its respective pods and applications
	based on Ingress rules (host and path). And we need Ingress controller in our cluster to read and process the Ingress rules.
	Ingress controller :
	Most used - Nginx Ingress controller, Happroxy, traefix, Istio

4]	Request Work-flow :
	http request -> Load Balancer -> Ingress Controller -> Ingress Rules (Read & Processed by Ingress Controller) -> Pod -> Application
	
5] 	Ingress rules types :

	a] Path-based routing - It allows you to route HTTP traffic to different services based on the path of the URL.
	This is useful when you have multiple services that serve different content or functionality based on the path.
	For example, you can have one service handle requests for /app1/* and another service handle requests for /app2/*.
	
	b] Host-based routing - It is also known as name-based virtual hosting, allows you to route HTTP traffic to different services
	based on the hostname in the URL. This is useful when you have multiple services that serve different content or
	functionality based on the hostname. For example, you can have one service handle requests for myservicea.foo.org and another service
	handle requests for myserviceb.foo.org.
	
	example from  - 15:00 - 22:00 ( Pavan Elthepu - Ingress )
	
6] 	Default backend :
	The default backend is a service that handles requests that do not match any specific path or host in the Ingress rules.
	This is useful when you want to provide a default response for unknown or unhandled requests, such as returning a 404 error or
	redirecting to a specific domain.
	If you don't specify a default backend in your Ingress configuration, the Ingress controller will typically provide a default backend that
	returns a 404 response.

7]	Configuring TLS certificate :
	A TLS (Transport Layer Security) certificate is a digital file that helps to authenticate and secure data transfers across the internet.
	It is used to establish a secure connection between a web server and a web browser, ensuring that data is transmitted privately and
	without modifications, loss, or theft.
	Types or ways for generating TLS certificates :
	a] Self-Signed Certificates
	b] Purchase an SSL Certificate
	c] Use Let's Encrypt Certificate
	d] Internal PKI Infrastructure

8]	Command used :
	minikube start --vm=true -p ingress-cluster				-> New cluster with VM driver for working with Ingress controller
	
{{ NOTE - We haven't Installed or worker with Virtual box or VM driver, we used Minikube with docker driver itself }}

	kubectl apply -f nginx-deployment.yaml
	kubectl apply -f nginx-service.yaml
	kubectl get all
	minikube addons enable ingress -p local-clus			-> We are adding Ingress controller to our local cluster
	
{{ NOTE - Ingress controller is an application inside a pod which is exposed by services }}
	
	kubectl get po -n ingress-nginx							-> Checkig wheather Ingress controller is enabled or not
	kubectl get svc -n ingress-nginx						-> Checkig wheather Ingress controller is enabled or not
		-> Here, Service assigned to ingress pod is NodePort service, when we use cloud environment it will be LoadBalancer service
	kubectl api-resources | grep Ingress
	kubectl apply -f nginx-ingress.yaml
	kubectl get ing											-> ing is short form for Ingress
	minikube ip -p local-clus								-> This gives us the IP address of our local cluster
	sudo vim /etc/hosts										-> editing hosts file by adding local cluster IP address mapped towards nginx-demo.com
	kubectl describe ing nginx-ingress
		-> Configured Default backend can be seen here
		
	openssl req -x509 -newkey rsa:4096 -sh256 -nodes -keyout tls.key -out tls.crt -subj "/CN=nginx-demo.com" -days 365
															-> Generating an Self-Signed certificate for 365 day

	kubectl create secret tls nginx-demo-com-tls --cert tls.crt --key tls.key			-> Generating an Secret
		-> Add the secret to ingress yaml file


NAMESPACE :
---------
Namespace is organizing various kubernetes objects associated with various applications, for easy management of cluster and sharing a single cluster
among varios teams, projects, customers, etc,..
a]	Namespace is a way to organise a cluster into virtual sub-cluster.
b]	When Kubernetes objects are created we can organise them within different sub-cluster.
c]	Any no of Namespaces is supported in a cluster, they are logically seperated from one another but they still can communicate with eachother.

1]	Need for Namespaces :
	a]	Avoiding conflicts - 
		In a single namespace we cant have same name of different kubernetes objects, which causes conflict issues.
		But, having different namespace we can create kubernetes objects with same names in different namespace.
	b] 	Restricting access -
		Providing access to only authorized person for production namespace, while giving other for access in development namespace.
	c]	Resource limits - Limiting resources of a namespace can help other applications in different namespace run smoothly.

2]	Default Namespaces and its types :
	The following Namespace is created when an Cluster is created.
	a] Default -
		Whenever any kubernetes objects are created, they will be created in Default Namespace only (if any specific Namespace is not mentioned).
	b] Kube-Node-Lease -
		Whenever an node goes down, the kubernetes cluster identifies it and moves it's respective pods to different node.
		It is achieved by lease objects, which sends heart-beat to cluster stating wheather nodes are running or not.
		Each node have their own associated lease objects. These objects are stored in kube-node-lease namespace.
	c] Kube-Public - 
		It is used for public resources. Where all have access to this namespace with only read permission.
	d] Kube-System -
		This namespace stores the objects created by kubernetes control plan.
	e] We can create our own Namespaces
	
3]	Commands used :
	kubectl create namespace nginx									-> Creating new Namespace
	kubectl get namespace
	kubectl api-resource | grep namespace
	kubectl delete ns nginx
	kubectl apply -f nginx-Namespace.yaml
	kubectl get namespace
	kubectl apply -f todo-Namespace.yaml
	kubectl get namespace
	kubectl apply -f nginx-deployment.yaml							-> Deployment is created in new Namespace
		-> Add " namespace: nginx " inside metadata: section
	kubectl get all -n nginx
	kubectl get all --all-namespaces								-> Kubernetes Objects from all Namespaces
	kubectl get all -A												-> Short version to get Kubernetes Objects from all Namespaces
	kubectl config set-context --current --namespace=nginx			-> Switching to different Namespace from Default Namespace
	curl todo-api-service:8080/api/todos							-> Inorder to access application if all service in Default Namespace
	curl todo-api-service.todo:8080/api/todos						-> Inorder to access application if a service in Different Namespace
		-> Just add .<Namespace name> inorder to access from that Namespace
		

VOLUMES :
-------
Volumes are common directories which can be accessed by all associated pods/containers. Volumes are introduced to solve below problems.

1]	Problems solved by Volumes :
	a] Data loss - When we apply Deployment.yaml or Replicaset.yaml, whenever an pod goes down another is created. But, Datas in old pod gets lost
	we it goes down, This resulting in Data loss.
	b] Data sharing - When replicas of same application is made, the data is stored in containers associated with them, there was no way to share
	data across pods/containers.

{{ NOTE - Above 2 Issues are observed at container level }}
	
2]	Types of Volumes :
	a] emptyDir Volume - Initially, The Data is stored in Container's directory, Which leads to above Problems.
	But, emptyDir is an common directory at pod level. Hence, by storing data in it will not result in above Problems.
	This solves issue at Container level, but above both ptoblem still exist on pod level.
	
	
	

	
2] 	Commands used :
	kubectl apply -f Deployment.yaml
		-> From Volumes directory
	kubectl get po
	kubectl port-forward svc/mongo-svc 32000:27017
	
	In Mongo Composs -> Creation of data
	
	kubectl exec -it mongo-78474469b4-rvjl9 -- /bin/bash			-> Entering pod, which doesn't work fr us
		-> By killing a process inside container we can observe above 2 mentioned problems.
	kubectl apply -f Deployment.yaml
		-> Added " volumeMounts:
						- mountPath: /data/db						-> Directory where data is stored in container
						  name: mongo-volume " and					-> Volume name
		   Added " volumes											-> Defining Volume
					- name: mongo-volume							-> Volume name
					emptyDir: {} "									-> Volume type
	kubectl get po
	kubectl port-forward svc/mongo-svc 32000:27017
	
	In Mongo Composs -> Creation of data
	
	kubectl exec -it mongo-78474469b4-rvjl9 -- /bin/bash
		-> By killing a process inside container we can observe above 2 mentioned problems resolved.
		   This solves issue at Container level, but above both ptoblem still exist on pod level.
	
	minikube ssh													-> Command allows users to access the Minikube environment for debugging purposes.
	sudo ls /var/lib/kubelet/pods									-> This directory is were emptyDir volume is stored (At Node)
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
HELM :
----
Helm helps you manage Kubernetes applications â€” Helm Charts help you define, install, and upgrade even the most complex Kubernetes application.


PROBLEMS SOLVED BY HELM :
-----------------------
1]	When we wanted to deploy an application in Kubernetes, We usually create and run lots of manifest files (Like deployment.yaml, service.yaml, etc)
	The complexity increases, as no of applications we deploy.
	-> Helm helps us to package or bundle all the manifest files required for an application, And allows us to run it in single go.
	   Chart is the term we use for package or bundle in Helm.

2]	In order to deploy an application, we need to deploy its dependencies first.
	(e.g) : For deploying an springboot application we needed to deploy Mongo DB first.
	-> But, with help of Helm we can instruct it to deploy Mongo DB first, Stating it as dependency before deploying our application.
	
3] 	Better configurability and deployability - When we wanted to deploy certain application in QA, Dev, etc,. environments, we need to change the
	environment variable values for each environments each time.
	-> But, with Helm we can assign placeholders in varible section, and we can change it accordingly using value.yaml files.
	
4]	When we deploy certain upgrade version to a application and wanted to rollback to previous version, we can rollback pods, deployments,
	services, etc,. But, rollbacking secrets is not possible using kubectl.
	-> But, with help of Helm we can rollback all objects (including secrets) to its previous version we needed.
	

HELM ARCHITECTURE :
-----------------
Helm is an Command line tool. When we use Helm cli, behind the scene it uses its library to prepare manifest file, interact with Kubernetes, 
And handle deployment smoothly. If needed it downloads charts from ArtifactHub (Artifact repo). It uses kubectl config file to establish its API
communication with Kubernetes (If required we can do configuration modifiactions).


DIFFERENT TYPES OF YAML FILES :
-----------------------------
1] 	Chart.yaml - Contains metadata of chart ( like name, version, etc,. )
	dependencies also to be added here (like mongo db dependencies)

2]	templates - Directory which contains Kubernetes manifest files, If we needed any manifest file to be added or removed for our application.
	We just need to add or delete them in templates directory.

3] 	values.yaml - In this file we give the values for the placeholders/parameters which we use in .yaml files inside templates.
	When we encounter "{{ value.--.---.  }}" in .yaml files, It means the value is obtained from values.yaml file.

4] -helper.yaml - reuseable templates are stored here (similar to method in JAVA)

GO TEMPLATING SYNTAX :
--------------------
Helm uses Go Templating syntax for templating, which is "{{ value.--.---.  }}"

	1] {{ .Values.image.tag | default .Chart.AppVersion }}  -> | default, which means if there is no value given in values.yaml file we need to 
	consider the default value.
	
	2] In Helm templates, {{ | quote }} is a built-in function that quotes a string value. It is commonly used to properly escape string values
	that will be inserted into YAML or JSON configuration files. The quote function simply wraps the string in double quotes and escapes any
	internal quotes or special characters.
	(e.g) :   myValue: {{ .Values.someValue | quote }}
	-> And the value of .Values.someValue is foo"bar, without quoting it would render as: myValue: foo"bar.
	   Which is invalid YAML. Using | quote renders it correctly as: myValue: "foo\"bar"
	   
	3] The range function in Helm templates allows you to iterate over a list or map and generate multiple resources based on the elements.
	It is similar to a "for each" loop in programming languages.
	{{- range .Values.ingress.tls }} -> It loops over the .Values.ingress.tls value from values.yaml file
	
	4] {{- if .Values.ingress.enabled -}} -> based on value of ingress in values.yaml file it gets enabled.
	
	5] nindent function
	
	6] {{-  -> clears free spaces
	
{{ NOTE - Chart template guide from Kubernetes documentations for more syntaxes }}
	
	
COMMAND USED :
------------
helm version
helm create todo-api											-> Creating an chart
	-> Creates a directory structure
helm lint .														-> To check if there is any errors
	-> It gives an error stating dependency chart available
helm dependency build											-> To build dependency
	-> mongo-db dependency built in charts directory
helm install todo-api .											-> To build our required application

............s
	
	
	
	

	


	
	
	
	
	
	
	
	






	
	
	
	


	
	
	


	
	
	

	




COMMANDS USED DURING SETUP :
--------------------------
minikube start --nodes 2 -p local-cluster --driver=docker
minikube status -p local-cluster
kubectl get nodes
docker ps
kubectl config get-contexts										-> Gets no of cluster, Current cluster represented by *
kubectl config set-context local-cluster						-> Switch between clusters (to local-cluster)
minikube node add --worker -p local-cluster						-> Adding an worker node to cluster (local-cluster)
minikube node delete <node name> -p local-cluster				-> Deleting an node from cluster (local-cluster)
minikube dashboard --url -p local-cluster						-> Gives URL of minikube dashboard


ISSUES FACED :
------------
"ErrImagePull" later into "ImagePullBackOff"
	-> After few mins it resolved on its own and now it's in "running" state.
	








